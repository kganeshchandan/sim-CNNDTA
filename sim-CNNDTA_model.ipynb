{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b53e326",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json,pickle,math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f12dc1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = pickle.load(open('./davis_ligand_similarity_matrix.pkl', 'rb'))\n",
    "PSM = pickle.load(open('./davis_protein_similarity_matrix.pkl', 'rb'))\n",
    "df = pd.read_csv(open('./davis_all_pairs.csv','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf827815",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3213da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES = json.load(open('./data/DAVIS/SMILES.txt'))\n",
    "TARGETS = json.load(open('./data/DAVIS/target_seq.txt'))\n",
    "SMILES=list(SMILES.values())\n",
    "TARGETS=list(TARGETS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1623a6b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30056, 1, 68, 442)\n"
     ]
    }
   ],
   "source": [
    "outer_prods = []\n",
    "for i,row in df.iterrows():\n",
    "#     print(i)\n",
    "    smi = row['SMILES']\n",
    "    seq = row['Target Sequence']\n",
    "    target_id = TARGETS.index(seq)\n",
    "    smi_id = SMILES.index(smi)\n",
    "    ki=LSM[smi_id]\n",
    "    kj=PSM[target_id]\n",
    "    ki_x_kj = np.outer(ki,kj)\n",
    "    outer_prods.append([ki_x_kj])\n",
    "outer_prods = np.array(outer_prods)\n",
    "print(np.shape(outer_prods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a9ee981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "# num_classes = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "511c953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file, outer_prods, transform=None):\n",
    "        self.df = pd.read_csv(open(csv_file))\n",
    "#         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.outer_prods = outer_prods\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        output = {'outer_product': self.outer_prods[idx] , 'Label':self.df.iloc[idx]['Label']}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1976b0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = custom_dataset(csv_file = './davis_all_pairs.csv', outer_prods = outer_prods)\n",
    "full_dataset = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d9fc732",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(full_dataset))\n",
    "test_size = len(full_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d18aa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader= torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf1a31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a6c9c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, 5).double()\n",
    "        self.pool1 = nn.MaxPool2d(2,2).double()\n",
    "        self.conv2 = nn.Conv2d(32,18,3).double()\n",
    "        self.pool2 = nn.MaxPool2d(2,2).double()\n",
    "        self.fc1 = nn.Linear(18*15*108, 128).double()\n",
    "        self.fc2 = nn.Linear(128,1).double()\n",
    "        self.dropout = nn.Dropout(0.1).double()\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1,18*15*108)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7170f7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_loader:\n",
    "#     a = i['outer_product']\n",
    "#     b= i['Label']\n",
    "#     break\n",
    "# conv1 = nn.Conv2d(1,32,5).double()\n",
    "# pool = nn.MaxPool2d(2,2).double()\n",
    "# conv2 = nn.Conv2d(32,18,3).double()\n",
    "# fc1 = nn.Linear(18*15*108, 128).double()\n",
    "# fc2 = nn.Linear(128,1).double()\n",
    "# dropout = nn.Dropout(0.1).double()\n",
    "# x= conv1(a)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x= conv2(x)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x = x.view(-1,18*15*108)\n",
    "# print(x.shape)\n",
    "# x = dropout(x)\n",
    "# print(x.shape)\n",
    "# x = fc1(x)\n",
    "# print(x.shape)\n",
    "# x = fc2(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c124bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "338f1174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "514bcab6",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7525adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,f):\n",
    "    rmse = math.sqrt(((y - f)**2).mean(axis=0))\n",
    "    return rmse\n",
    "def mse(y,f):\n",
    "    mse = ((y - f)**2).mean(axis=0)\n",
    "    return mse\n",
    "def pearson(y,f):\n",
    "    rp = np.corrcoef(y, f)[0,1]\n",
    "    return rp\n",
    "from lifelines.utils import concordance_index\n",
    "def ci(y,f):\n",
    "    return concordance_index(y,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4297e4aa",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1236b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_preds = np.array([])\n",
    "    total_labels = np.array([])\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i in test_loader:\n",
    "            images = i['outer_product']\n",
    "            labels = i['Label']\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) \n",
    "            outputs = outputs.cpu().detach().numpy().flatten()\n",
    "            labels =labels.cpu().detach().numpy().flatten()\n",
    "            P = np.concatenate([total_preds, outputs])\n",
    "            G = np.concatenate([total_labels, labels])\n",
    "        \n",
    "    return G, P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07694279",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kanakala.ganesh/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/752], Loss: 30.7756\n",
      "Epoch [1/20], Step [2/752], Loss: 13.8066\n",
      "Epoch [1/20], Step [3/752], Loss: 2.6147\n",
      "Epoch [1/20], Step [4/752], Loss: 6.5571\n",
      "Epoch [1/20], Step [5/752], Loss: 5.9739\n",
      "Epoch [1/20], Step [6/752], Loss: 2.2430\n",
      "Epoch [1/20], Step [7/752], Loss: 0.7734\n",
      "Epoch [1/20], Step [8/752], Loss: 2.2827\n",
      "Epoch [1/20], Step [9/752], Loss: 2.3536\n",
      "Epoch [1/20], Step [10/752], Loss: 3.2405\n",
      "Epoch [1/20], Step [11/752], Loss: 1.7166\n",
      "Epoch [1/20], Step [12/752], Loss: 1.0915\n",
      "Epoch [1/20], Step [13/752], Loss: 0.4333\n",
      "Epoch [1/20], Step [14/752], Loss: 0.9320\n",
      "Epoch [1/20], Step [15/752], Loss: 1.8298\n",
      "Epoch [1/20], Step [16/752], Loss: 1.3822\n",
      "Epoch [1/20], Step [17/752], Loss: 0.7746\n",
      "Epoch [1/20], Step [18/752], Loss: 0.4310\n",
      "Epoch [1/20], Step [19/752], Loss: 1.6516\n",
      "Epoch [1/20], Step [20/752], Loss: 1.2584\n",
      "Epoch [1/20], Step [21/752], Loss: 0.9037\n",
      "Epoch [1/20], Step [22/752], Loss: 2.5983\n",
      "Epoch [1/20], Step [23/752], Loss: 0.5482\n",
      "Epoch [1/20], Step [24/752], Loss: 0.7040\n",
      "Epoch [1/20], Step [25/752], Loss: 0.5817\n",
      "Epoch [1/20], Step [26/752], Loss: 0.5676\n",
      "Epoch [1/20], Step [27/752], Loss: 0.6700\n",
      "Epoch [1/20], Step [28/752], Loss: 0.7291\n",
      "Epoch [1/20], Step [29/752], Loss: 0.8874\n",
      "Epoch [1/20], Step [30/752], Loss: 1.6533\n",
      "Epoch [1/20], Step [31/752], Loss: 0.6586\n",
      "Epoch [1/20], Step [32/752], Loss: 1.2837\n",
      "Epoch [1/20], Step [33/752], Loss: 0.8605\n",
      "Epoch [1/20], Step [34/752], Loss: 0.8742\n",
      "Epoch [1/20], Step [35/752], Loss: 0.5811\n",
      "Epoch [1/20], Step [36/752], Loss: 0.7517\n",
      "Epoch [1/20], Step [37/752], Loss: 0.6285\n",
      "Epoch [1/20], Step [38/752], Loss: 1.4058\n",
      "Epoch [1/20], Step [39/752], Loss: 0.4370\n",
      "Epoch [1/20], Step [40/752], Loss: 0.7854\n",
      "Epoch [1/20], Step [41/752], Loss: 0.8322\n",
      "Epoch [1/20], Step [42/752], Loss: 1.2141\n",
      "Epoch [1/20], Step [43/752], Loss: 1.3140\n",
      "Epoch [1/20], Step [44/752], Loss: 0.6223\n",
      "Epoch [1/20], Step [45/752], Loss: 0.3375\n",
      "Epoch [1/20], Step [46/752], Loss: 1.2513\n",
      "Epoch [1/20], Step [47/752], Loss: 1.2338\n",
      "Epoch [1/20], Step [48/752], Loss: 1.6310\n",
      "Epoch [1/20], Step [49/752], Loss: 0.9859\n",
      "Epoch [1/20], Step [50/752], Loss: 0.7996\n",
      "Epoch [1/20], Step [51/752], Loss: 0.8634\n",
      "Epoch [1/20], Step [52/752], Loss: 0.5510\n",
      "Epoch [1/20], Step [53/752], Loss: 0.5814\n",
      "Epoch [1/20], Step [54/752], Loss: 0.8614\n",
      "Epoch [1/20], Step [55/752], Loss: 1.2618\n",
      "Epoch [1/20], Step [56/752], Loss: 1.0253\n",
      "Epoch [1/20], Step [57/752], Loss: 0.4944\n",
      "Epoch [1/20], Step [58/752], Loss: 0.2405\n",
      "Epoch [1/20], Step [59/752], Loss: 0.6375\n",
      "Epoch [1/20], Step [60/752], Loss: 0.4850\n",
      "Epoch [1/20], Step [61/752], Loss: 1.0807\n",
      "Epoch [1/20], Step [62/752], Loss: 0.8809\n",
      "Epoch [1/20], Step [63/752], Loss: 0.9037\n",
      "Epoch [1/20], Step [64/752], Loss: 0.8716\n",
      "Epoch [1/20], Step [65/752], Loss: 0.5926\n",
      "Epoch [1/20], Step [66/752], Loss: 1.0030\n",
      "Epoch [1/20], Step [67/752], Loss: 0.7910\n",
      "Epoch [1/20], Step [68/752], Loss: 0.7550\n",
      "Epoch [1/20], Step [69/752], Loss: 0.8464\n",
      "Epoch [1/20], Step [70/752], Loss: 0.5115\n",
      "Epoch [1/20], Step [71/752], Loss: 0.4253\n",
      "Epoch [1/20], Step [72/752], Loss: 0.7344\n",
      "Epoch [1/20], Step [73/752], Loss: 0.8365\n",
      "Epoch [1/20], Step [74/752], Loss: 0.5784\n",
      "Epoch [1/20], Step [75/752], Loss: 0.7777\n",
      "Epoch [1/20], Step [76/752], Loss: 0.7594\n",
      "Epoch [1/20], Step [77/752], Loss: 0.9413\n",
      "Epoch [1/20], Step [78/752], Loss: 1.0972\n",
      "Epoch [1/20], Step [79/752], Loss: 0.5654\n",
      "Epoch [1/20], Step [80/752], Loss: 0.6186\n",
      "Epoch [1/20], Step [81/752], Loss: 0.7763\n",
      "Epoch [1/20], Step [82/752], Loss: 1.2295\n",
      "Epoch [1/20], Step [83/752], Loss: 0.4965\n",
      "Epoch [1/20], Step [84/752], Loss: 0.5992\n",
      "Epoch [1/20], Step [85/752], Loss: 0.3303\n",
      "Epoch [1/20], Step [86/752], Loss: 0.9541\n",
      "Epoch [1/20], Step [87/752], Loss: 0.2185\n",
      "Epoch [1/20], Step [88/752], Loss: 0.9646\n",
      "Epoch [1/20], Step [89/752], Loss: 0.7008\n",
      "Epoch [1/20], Step [90/752], Loss: 0.9322\n",
      "Epoch [1/20], Step [91/752], Loss: 0.8082\n",
      "Epoch [1/20], Step [92/752], Loss: 0.6184\n",
      "Epoch [1/20], Step [93/752], Loss: 0.5924\n",
      "Epoch [1/20], Step [94/752], Loss: 0.7161\n",
      "Epoch [1/20], Step [95/752], Loss: 1.0558\n",
      "Epoch [1/20], Step [96/752], Loss: 0.3933\n",
      "Epoch [1/20], Step [97/752], Loss: 0.7293\n",
      "Epoch [1/20], Step [98/752], Loss: 0.3690\n",
      "Epoch [1/20], Step [99/752], Loss: 0.7034\n",
      "Epoch [1/20], Step [100/752], Loss: 0.1595\n",
      "Epoch [1/20], Step [101/752], Loss: 0.6789\n",
      "Epoch [1/20], Step [102/752], Loss: 1.4567\n",
      "Epoch [1/20], Step [103/752], Loss: 1.5300\n",
      "Epoch [1/20], Step [104/752], Loss: 1.2107\n",
      "Epoch [1/20], Step [105/752], Loss: 1.1580\n",
      "Epoch [1/20], Step [106/752], Loss: 1.1501\n",
      "Epoch [1/20], Step [107/752], Loss: 0.6649\n",
      "Epoch [1/20], Step [108/752], Loss: 1.0035\n",
      "Epoch [1/20], Step [109/752], Loss: 0.7613\n",
      "Epoch [1/20], Step [110/752], Loss: 1.1609\n",
      "Epoch [1/20], Step [111/752], Loss: 0.1677\n",
      "Epoch [1/20], Step [112/752], Loss: 1.0553\n",
      "Epoch [1/20], Step [113/752], Loss: 0.6165\n",
      "Epoch [1/20], Step [114/752], Loss: 1.0239\n",
      "Epoch [1/20], Step [115/752], Loss: 1.0646\n",
      "Epoch [1/20], Step [116/752], Loss: 0.9417\n",
      "Epoch [1/20], Step [117/752], Loss: 0.5271\n",
      "Epoch [1/20], Step [118/752], Loss: 0.5546\n",
      "Epoch [1/20], Step [119/752], Loss: 0.5711\n",
      "Epoch [1/20], Step [120/752], Loss: 1.1095\n",
      "Epoch [1/20], Step [121/752], Loss: 1.0445\n",
      "Epoch [1/20], Step [122/752], Loss: 1.0091\n",
      "Epoch [1/20], Step [123/752], Loss: 0.3383\n",
      "Epoch [1/20], Step [124/752], Loss: 1.0399\n",
      "Epoch [1/20], Step [125/752], Loss: 0.9055\n",
      "Epoch [1/20], Step [126/752], Loss: 1.3252\n",
      "Epoch [1/20], Step [127/752], Loss: 0.7708\n",
      "Epoch [1/20], Step [128/752], Loss: 0.9816\n",
      "Epoch [1/20], Step [129/752], Loss: 0.4854\n",
      "Epoch [1/20], Step [130/752], Loss: 0.8024\n",
      "Epoch [1/20], Step [131/752], Loss: 0.7293\n",
      "Epoch [1/20], Step [132/752], Loss: 0.4006\n",
      "Epoch [1/20], Step [133/752], Loss: 0.6698\n",
      "Epoch [1/20], Step [134/752], Loss: 0.2720\n",
      "Epoch [1/20], Step [135/752], Loss: 1.0257\n",
      "Epoch [1/20], Step [136/752], Loss: 0.7456\n",
      "Epoch [1/20], Step [137/752], Loss: 0.3756\n",
      "Epoch [1/20], Step [138/752], Loss: 1.4681\n",
      "Epoch [1/20], Step [139/752], Loss: 0.6991\n",
      "Epoch [1/20], Step [140/752], Loss: 0.7254\n",
      "Epoch [1/20], Step [141/752], Loss: 0.5199\n",
      "Epoch [1/20], Step [142/752], Loss: 0.8551\n",
      "Epoch [1/20], Step [143/752], Loss: 0.7344\n",
      "Epoch [1/20], Step [144/752], Loss: 0.3090\n",
      "Epoch [1/20], Step [145/752], Loss: 1.3049\n",
      "Epoch [1/20], Step [146/752], Loss: 0.3771\n",
      "Epoch [1/20], Step [147/752], Loss: 0.7142\n",
      "Epoch [1/20], Step [148/752], Loss: 0.4840\n",
      "Epoch [1/20], Step [149/752], Loss: 0.6528\n",
      "Epoch [1/20], Step [150/752], Loss: 0.4901\n",
      "Epoch [1/20], Step [151/752], Loss: 0.6408\n",
      "Epoch [1/20], Step [152/752], Loss: 0.7917\n",
      "Epoch [1/20], Step [153/752], Loss: 1.0020\n",
      "Epoch [1/20], Step [154/752], Loss: 0.7961\n",
      "Epoch [1/20], Step [155/752], Loss: 0.9389\n",
      "Epoch [1/20], Step [156/752], Loss: 0.5192\n",
      "Epoch [1/20], Step [157/752], Loss: 1.1006\n",
      "Epoch [1/20], Step [158/752], Loss: 0.8344\n",
      "Epoch [1/20], Step [159/752], Loss: 0.9511\n",
      "Epoch [1/20], Step [160/752], Loss: 0.6991\n",
      "Epoch [1/20], Step [161/752], Loss: 0.8690\n",
      "Epoch [1/20], Step [162/752], Loss: 0.6378\n",
      "Epoch [1/20], Step [163/752], Loss: 0.7326\n",
      "Epoch [1/20], Step [164/752], Loss: 1.4976\n",
      "Epoch [1/20], Step [165/752], Loss: 0.6643\n",
      "Epoch [1/20], Step [166/752], Loss: 0.6498\n",
      "Epoch [1/20], Step [167/752], Loss: 0.7493\n",
      "Epoch [1/20], Step [168/752], Loss: 0.3386\n",
      "Epoch [1/20], Step [169/752], Loss: 0.5433\n",
      "Epoch [1/20], Step [170/752], Loss: 1.4454\n",
      "Epoch [1/20], Step [171/752], Loss: 0.7257\n",
      "Epoch [1/20], Step [172/752], Loss: 0.5793\n",
      "Epoch [1/20], Step [173/752], Loss: 0.7435\n",
      "Epoch [1/20], Step [174/752], Loss: 0.5313\n",
      "Epoch [1/20], Step [175/752], Loss: 0.8569\n",
      "Epoch [1/20], Step [176/752], Loss: 0.1758\n",
      "Epoch [1/20], Step [177/752], Loss: 2.0737\n",
      "Epoch [1/20], Step [178/752], Loss: 1.4217\n",
      "Epoch [1/20], Step [179/752], Loss: 0.3136\n",
      "Epoch [1/20], Step [180/752], Loss: 0.4799\n",
      "Epoch [1/20], Step [181/752], Loss: 0.9434\n",
      "Epoch [1/20], Step [182/752], Loss: 1.1815\n",
      "Epoch [1/20], Step [183/752], Loss: 0.3200\n",
      "Epoch [1/20], Step [184/752], Loss: 0.8070\n",
      "Epoch [1/20], Step [185/752], Loss: 0.5816\n",
      "Epoch [1/20], Step [186/752], Loss: 0.6996\n",
      "Epoch [1/20], Step [187/752], Loss: 0.5525\n",
      "Epoch [1/20], Step [188/752], Loss: 1.0856\n",
      "Epoch [1/20], Step [189/752], Loss: 0.3293\n",
      "Epoch [1/20], Step [190/752], Loss: 0.4750\n",
      "Epoch [1/20], Step [191/752], Loss: 0.8175\n",
      "Epoch [1/20], Step [192/752], Loss: 0.8789\n",
      "Epoch [1/20], Step [193/752], Loss: 0.7165\n",
      "Epoch [1/20], Step [194/752], Loss: 1.1407\n",
      "Epoch [1/20], Step [195/752], Loss: 1.0743\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [196/752], Loss: 0.4235\n",
      "Epoch [1/20], Step [197/752], Loss: 0.8382\n",
      "Epoch [1/20], Step [198/752], Loss: 0.9811\n",
      "Epoch [1/20], Step [199/752], Loss: 1.0040\n",
      "Epoch [1/20], Step [200/752], Loss: 1.3425\n",
      "Epoch [1/20], Step [201/752], Loss: 0.8560\n",
      "Epoch [1/20], Step [202/752], Loss: 0.4847\n",
      "Epoch [1/20], Step [203/752], Loss: 0.4832\n",
      "Epoch [1/20], Step [204/752], Loss: 1.0369\n",
      "Epoch [1/20], Step [205/752], Loss: 0.7190\n",
      "Epoch [1/20], Step [206/752], Loss: 0.9051\n",
      "Epoch [1/20], Step [207/752], Loss: 0.5461\n",
      "Epoch [1/20], Step [208/752], Loss: 0.4314\n",
      "Epoch [1/20], Step [209/752], Loss: 0.4025\n",
      "Epoch [1/20], Step [210/752], Loss: 1.0628\n",
      "Epoch [1/20], Step [211/752], Loss: 0.8918\n",
      "Epoch [1/20], Step [212/752], Loss: 0.8389\n",
      "Epoch [1/20], Step [213/752], Loss: 1.1772\n",
      "Epoch [1/20], Step [214/752], Loss: 1.2852\n",
      "Epoch [1/20], Step [215/752], Loss: 1.0129\n",
      "Epoch [1/20], Step [216/752], Loss: 0.7579\n",
      "Epoch [1/20], Step [217/752], Loss: 1.4654\n",
      "Epoch [1/20], Step [218/752], Loss: 0.7323\n",
      "Epoch [1/20], Step [219/752], Loss: 0.9533\n",
      "Epoch [1/20], Step [220/752], Loss: 0.5487\n",
      "Epoch [1/20], Step [221/752], Loss: 0.6440\n",
      "Epoch [1/20], Step [222/752], Loss: 0.6205\n",
      "Epoch [1/20], Step [223/752], Loss: 0.2703\n",
      "Epoch [1/20], Step [224/752], Loss: 1.0531\n",
      "Epoch [1/20], Step [225/752], Loss: 0.7818\n",
      "Epoch [1/20], Step [226/752], Loss: 0.2321\n",
      "Epoch [1/20], Step [227/752], Loss: 0.2818\n",
      "Epoch [1/20], Step [228/752], Loss: 0.2974\n",
      "Epoch [1/20], Step [229/752], Loss: 0.4396\n",
      "Epoch [1/20], Step [230/752], Loss: 0.8919\n",
      "Epoch [1/20], Step [231/752], Loss: 0.3789\n",
      "Epoch [1/20], Step [232/752], Loss: 0.2636\n",
      "Epoch [1/20], Step [233/752], Loss: 0.2736\n",
      "Epoch [1/20], Step [234/752], Loss: 0.7688\n",
      "Epoch [1/20], Step [235/752], Loss: 0.4031\n",
      "Epoch [1/20], Step [236/752], Loss: 0.8918\n",
      "Epoch [1/20], Step [237/752], Loss: 0.9825\n",
      "Epoch [1/20], Step [238/752], Loss: 0.9791\n",
      "Epoch [1/20], Step [239/752], Loss: 1.0884\n",
      "Epoch [1/20], Step [240/752], Loss: 0.7622\n",
      "Epoch [1/20], Step [241/752], Loss: 0.5846\n",
      "Epoch [1/20], Step [242/752], Loss: 0.5776\n",
      "Epoch [1/20], Step [243/752], Loss: 0.3716\n",
      "Epoch [1/20], Step [244/752], Loss: 1.5216\n",
      "Epoch [1/20], Step [245/752], Loss: 0.4618\n",
      "Epoch [1/20], Step [246/752], Loss: 0.5121\n",
      "Epoch [1/20], Step [247/752], Loss: 0.9533\n",
      "Epoch [1/20], Step [248/752], Loss: 1.1203\n",
      "Epoch [1/20], Step [249/752], Loss: 0.5209\n",
      "Epoch [1/20], Step [250/752], Loss: 0.5011\n",
      "Epoch [1/20], Step [251/752], Loss: 0.9789\n",
      "Epoch [1/20], Step [252/752], Loss: 0.9337\n",
      "Epoch [1/20], Step [253/752], Loss: 1.5311\n",
      "Epoch [1/20], Step [254/752], Loss: 0.5386\n",
      "Epoch [1/20], Step [255/752], Loss: 0.7210\n",
      "Epoch [1/20], Step [256/752], Loss: 0.7662\n",
      "Epoch [1/20], Step [257/752], Loss: 0.7403\n",
      "Epoch [1/20], Step [258/752], Loss: 0.8969\n",
      "Epoch [1/20], Step [259/752], Loss: 1.0232\n",
      "Epoch [1/20], Step [260/752], Loss: 0.9385\n",
      "Epoch [1/20], Step [261/752], Loss: 0.7816\n",
      "Epoch [1/20], Step [262/752], Loss: 0.6933\n",
      "Epoch [1/20], Step [263/752], Loss: 0.6016\n",
      "Epoch [1/20], Step [264/752], Loss: 0.4402\n",
      "Epoch [1/20], Step [265/752], Loss: 0.3677\n",
      "Epoch [1/20], Step [266/752], Loss: 0.3470\n",
      "Epoch [1/20], Step [267/752], Loss: 0.6971\n",
      "Epoch [1/20], Step [268/752], Loss: 0.3570\n",
      "Epoch [1/20], Step [269/752], Loss: 0.1724\n",
      "Epoch [1/20], Step [270/752], Loss: 0.2960\n",
      "Epoch [1/20], Step [271/752], Loss: 0.3254\n",
      "Epoch [1/20], Step [272/752], Loss: 0.3482\n",
      "Epoch [1/20], Step [273/752], Loss: 0.5708\n",
      "Epoch [1/20], Step [274/752], Loss: 0.8231\n",
      "Epoch [1/20], Step [275/752], Loss: 0.5949\n",
      "Epoch [1/20], Step [276/752], Loss: 0.4455\n",
      "Epoch [1/20], Step [277/752], Loss: 0.7714\n",
      "Epoch [1/20], Step [278/752], Loss: 0.5528\n",
      "Epoch [1/20], Step [279/752], Loss: 0.7856\n",
      "Epoch [1/20], Step [280/752], Loss: 0.3231\n",
      "Epoch [1/20], Step [281/752], Loss: 0.2891\n",
      "Epoch [1/20], Step [282/752], Loss: 0.2466\n",
      "Epoch [1/20], Step [283/752], Loss: 0.9920\n",
      "Epoch [1/20], Step [284/752], Loss: 0.5626\n",
      "Epoch [1/20], Step [285/752], Loss: 1.3442\n",
      "Epoch [1/20], Step [286/752], Loss: 0.2736\n",
      "Epoch [1/20], Step [287/752], Loss: 0.8263\n",
      "Epoch [1/20], Step [288/752], Loss: 0.5318\n",
      "Epoch [1/20], Step [289/752], Loss: 0.6105\n",
      "Epoch [1/20], Step [290/752], Loss: 0.9398\n",
      "Epoch [1/20], Step [291/752], Loss: 0.9427\n",
      "Epoch [1/20], Step [292/752], Loss: 0.5773\n",
      "Epoch [1/20], Step [293/752], Loss: 0.9367\n",
      "Epoch [1/20], Step [294/752], Loss: 0.8692\n",
      "Epoch [1/20], Step [295/752], Loss: 0.3387\n",
      "Epoch [1/20], Step [296/752], Loss: 0.5489\n",
      "Epoch [1/20], Step [297/752], Loss: 0.6717\n",
      "Epoch [1/20], Step [298/752], Loss: 0.2949\n",
      "Epoch [1/20], Step [299/752], Loss: 0.9751\n",
      "Epoch [1/20], Step [300/752], Loss: 0.3881\n",
      "Epoch [1/20], Step [301/752], Loss: 0.8543\n",
      "Epoch [1/20], Step [302/752], Loss: 1.0419\n",
      "Epoch [1/20], Step [303/752], Loss: 0.6214\n",
      "Epoch [1/20], Step [304/752], Loss: 0.4998\n",
      "Epoch [1/20], Step [305/752], Loss: 0.3501\n",
      "Epoch [1/20], Step [306/752], Loss: 0.4587\n",
      "Epoch [1/20], Step [307/752], Loss: 0.3160\n",
      "Epoch [1/20], Step [308/752], Loss: 0.9574\n",
      "Epoch [1/20], Step [309/752], Loss: 1.0124\n",
      "Epoch [1/20], Step [310/752], Loss: 1.0900\n",
      "Epoch [1/20], Step [311/752], Loss: 0.8301\n",
      "Epoch [1/20], Step [312/752], Loss: 0.5751\n",
      "Epoch [1/20], Step [313/752], Loss: 1.0661\n",
      "Epoch [1/20], Step [314/752], Loss: 1.1454\n",
      "Epoch [1/20], Step [315/752], Loss: 0.8045\n",
      "Epoch [1/20], Step [316/752], Loss: 0.6243\n",
      "Epoch [1/20], Step [317/752], Loss: 0.7682\n",
      "Epoch [1/20], Step [318/752], Loss: 0.3815\n",
      "Epoch [1/20], Step [319/752], Loss: 1.0653\n",
      "Epoch [1/20], Step [320/752], Loss: 0.5985\n",
      "Epoch [1/20], Step [321/752], Loss: 0.2662\n",
      "Epoch [1/20], Step [322/752], Loss: 0.4710\n",
      "Epoch [1/20], Step [323/752], Loss: 0.7241\n",
      "Epoch [1/20], Step [324/752], Loss: 0.8221\n",
      "Epoch [1/20], Step [325/752], Loss: 0.2614\n",
      "Epoch [1/20], Step [326/752], Loss: 0.3089\n",
      "Epoch [1/20], Step [327/752], Loss: 1.1622\n",
      "Epoch [1/20], Step [328/752], Loss: 1.0207\n",
      "Epoch [1/20], Step [329/752], Loss: 0.7396\n",
      "Epoch [1/20], Step [330/752], Loss: 0.8951\n",
      "Epoch [1/20], Step [331/752], Loss: 0.6193\n",
      "Epoch [1/20], Step [332/752], Loss: 1.0183\n",
      "Epoch [1/20], Step [333/752], Loss: 0.5365\n",
      "Epoch [1/20], Step [334/752], Loss: 0.4034\n",
      "Epoch [1/20], Step [335/752], Loss: 0.2978\n",
      "Epoch [1/20], Step [336/752], Loss: 0.3718\n",
      "Epoch [1/20], Step [337/752], Loss: 0.1297\n",
      "Epoch [1/20], Step [338/752], Loss: 0.6864\n",
      "Epoch [1/20], Step [339/752], Loss: 0.7507\n",
      "Epoch [1/20], Step [340/752], Loss: 0.3914\n",
      "Epoch [1/20], Step [341/752], Loss: 0.8667\n",
      "Epoch [1/20], Step [342/752], Loss: 0.3980\n",
      "Epoch [1/20], Step [343/752], Loss: 0.2587\n",
      "Epoch [1/20], Step [344/752], Loss: 0.4031\n",
      "Epoch [1/20], Step [345/752], Loss: 0.7977\n",
      "Epoch [1/20], Step [346/752], Loss: 0.4966\n",
      "Epoch [1/20], Step [347/752], Loss: 0.2967\n",
      "Epoch [1/20], Step [348/752], Loss: 0.5134\n",
      "Epoch [1/20], Step [349/752], Loss: 0.2889\n",
      "Epoch [1/20], Step [350/752], Loss: 0.5520\n",
      "Epoch [1/20], Step [351/752], Loss: 0.3967\n",
      "Epoch [1/20], Step [352/752], Loss: 0.9291\n",
      "Epoch [1/20], Step [353/752], Loss: 1.2318\n",
      "Epoch [1/20], Step [354/752], Loss: 0.6611\n",
      "Epoch [1/20], Step [355/752], Loss: 0.6035\n",
      "Epoch [1/20], Step [356/752], Loss: 0.7785\n",
      "Epoch [1/20], Step [357/752], Loss: 0.5710\n",
      "Epoch [1/20], Step [358/752], Loss: 0.7869\n",
      "Epoch [1/20], Step [359/752], Loss: 0.9029\n",
      "Epoch [1/20], Step [360/752], Loss: 0.5828\n",
      "Epoch [1/20], Step [361/752], Loss: 0.5145\n",
      "Epoch [1/20], Step [362/752], Loss: 0.4920\n",
      "Epoch [1/20], Step [363/752], Loss: 0.6916\n",
      "Epoch [1/20], Step [364/752], Loss: 0.9739\n",
      "Epoch [1/20], Step [365/752], Loss: 0.7712\n",
      "Epoch [1/20], Step [366/752], Loss: 0.7836\n",
      "Epoch [1/20], Step [367/752], Loss: 0.8743\n",
      "Epoch [1/20], Step [368/752], Loss: 0.3440\n",
      "Epoch [1/20], Step [369/752], Loss: 0.2702\n",
      "Epoch [1/20], Step [370/752], Loss: 1.0921\n",
      "Epoch [1/20], Step [371/752], Loss: 0.4839\n",
      "Epoch [1/20], Step [372/752], Loss: 1.6333\n",
      "Epoch [1/20], Step [373/752], Loss: 0.9423\n",
      "Epoch [1/20], Step [374/752], Loss: 1.1893\n",
      "Epoch [1/20], Step [375/752], Loss: 1.1849\n",
      "Epoch [1/20], Step [376/752], Loss: 0.9314\n",
      "Epoch [1/20], Step [377/752], Loss: 0.7460\n",
      "Epoch [1/20], Step [378/752], Loss: 0.5708\n",
      "Epoch [1/20], Step [379/752], Loss: 0.4180\n",
      "Epoch [1/20], Step [380/752], Loss: 1.4967\n",
      "Epoch [1/20], Step [381/752], Loss: 1.0530\n",
      "Epoch [1/20], Step [382/752], Loss: 0.5622\n",
      "Epoch [1/20], Step [383/752], Loss: 0.8948\n",
      "Epoch [1/20], Step [384/752], Loss: 0.6522\n",
      "Epoch [1/20], Step [385/752], Loss: 0.2814\n",
      "Epoch [1/20], Step [386/752], Loss: 0.5189\n",
      "Epoch [1/20], Step [387/752], Loss: 0.7675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [388/752], Loss: 0.9697\n",
      "Epoch [1/20], Step [389/752], Loss: 0.5665\n",
      "Epoch [1/20], Step [390/752], Loss: 0.6193\n",
      "Epoch [1/20], Step [391/752], Loss: 0.4722\n",
      "Epoch [1/20], Step [392/752], Loss: 0.7818\n",
      "Epoch [1/20], Step [393/752], Loss: 0.6756\n",
      "Epoch [1/20], Step [394/752], Loss: 0.4900\n",
      "Epoch [1/20], Step [395/752], Loss: 0.9045\n",
      "Epoch [1/20], Step [396/752], Loss: 1.2896\n",
      "Epoch [1/20], Step [397/752], Loss: 0.5498\n",
      "Epoch [1/20], Step [398/752], Loss: 0.7765\n",
      "Epoch [1/20], Step [399/752], Loss: 0.4028\n",
      "Epoch [1/20], Step [400/752], Loss: 0.8440\n",
      "Epoch [1/20], Step [401/752], Loss: 1.0463\n",
      "Epoch [1/20], Step [402/752], Loss: 0.6522\n",
      "Epoch [1/20], Step [403/752], Loss: 0.2990\n",
      "Epoch [1/20], Step [404/752], Loss: 0.4708\n",
      "Epoch [1/20], Step [405/752], Loss: 1.4705\n",
      "Epoch [1/20], Step [406/752], Loss: 1.0521\n",
      "Epoch [1/20], Step [407/752], Loss: 0.3692\n",
      "Epoch [1/20], Step [408/752], Loss: 0.3718\n",
      "Epoch [1/20], Step [409/752], Loss: 0.8507\n",
      "Epoch [1/20], Step [410/752], Loss: 0.5772\n",
      "Epoch [1/20], Step [411/752], Loss: 0.9261\n",
      "Epoch [1/20], Step [412/752], Loss: 0.6152\n",
      "Epoch [1/20], Step [413/752], Loss: 0.3247\n",
      "Epoch [1/20], Step [414/752], Loss: 0.4877\n",
      "Epoch [1/20], Step [415/752], Loss: 0.9722\n",
      "Epoch [1/20], Step [416/752], Loss: 0.6969\n",
      "Epoch [1/20], Step [417/752], Loss: 1.6705\n",
      "Epoch [1/20], Step [418/752], Loss: 0.6549\n",
      "Epoch [1/20], Step [419/752], Loss: 0.8394\n",
      "Epoch [1/20], Step [420/752], Loss: 0.5713\n",
      "Epoch [1/20], Step [421/752], Loss: 0.3175\n",
      "Epoch [1/20], Step [422/752], Loss: 0.5313\n",
      "Epoch [1/20], Step [423/752], Loss: 1.0938\n",
      "Epoch [1/20], Step [424/752], Loss: 0.5850\n",
      "Epoch [1/20], Step [425/752], Loss: 0.5751\n",
      "Epoch [1/20], Step [426/752], Loss: 1.5744\n",
      "Epoch [1/20], Step [427/752], Loss: 0.6551\n",
      "Epoch [1/20], Step [428/752], Loss: 1.0931\n",
      "Epoch [1/20], Step [429/752], Loss: 1.1125\n",
      "Epoch [1/20], Step [430/752], Loss: 0.5449\n",
      "Epoch [1/20], Step [431/752], Loss: 0.9628\n",
      "Epoch [1/20], Step [432/752], Loss: 0.8094\n",
      "Epoch [1/20], Step [433/752], Loss: 0.4580\n",
      "Epoch [1/20], Step [434/752], Loss: 0.5103\n",
      "Epoch [1/20], Step [435/752], Loss: 0.6404\n",
      "Epoch [1/20], Step [436/752], Loss: 0.6472\n",
      "Epoch [1/20], Step [437/752], Loss: 0.5183\n",
      "Epoch [1/20], Step [438/752], Loss: 0.9507\n",
      "Epoch [1/20], Step [439/752], Loss: 1.0779\n",
      "Epoch [1/20], Step [440/752], Loss: 0.7063\n",
      "Epoch [1/20], Step [441/752], Loss: 1.4011\n",
      "Epoch [1/20], Step [442/752], Loss: 0.5616\n",
      "Epoch [1/20], Step [443/752], Loss: 0.5220\n",
      "Epoch [1/20], Step [444/752], Loss: 1.1059\n",
      "Epoch [1/20], Step [445/752], Loss: 0.2592\n",
      "Epoch [1/20], Step [446/752], Loss: 0.3455\n",
      "Epoch [1/20], Step [447/752], Loss: 0.6015\n",
      "Epoch [1/20], Step [448/752], Loss: 0.7940\n",
      "Epoch [1/20], Step [449/752], Loss: 0.5354\n",
      "Epoch [1/20], Step [450/752], Loss: 0.8824\n",
      "Epoch [1/20], Step [451/752], Loss: 0.6129\n",
      "Epoch [1/20], Step [452/752], Loss: 0.5636\n",
      "Epoch [1/20], Step [453/752], Loss: 0.4582\n",
      "Epoch [1/20], Step [454/752], Loss: 0.7922\n",
      "Epoch [1/20], Step [455/752], Loss: 0.5586\n",
      "Epoch [1/20], Step [456/752], Loss: 0.2622\n",
      "Epoch [1/20], Step [457/752], Loss: 0.3662\n",
      "Epoch [1/20], Step [458/752], Loss: 0.8632\n",
      "Epoch [1/20], Step [459/752], Loss: 0.5759\n",
      "Epoch [1/20], Step [460/752], Loss: 0.3514\n",
      "Epoch [1/20], Step [461/752], Loss: 0.4755\n",
      "Epoch [1/20], Step [462/752], Loss: 0.4577\n",
      "Epoch [1/20], Step [463/752], Loss: 0.4371\n",
      "Epoch [1/20], Step [464/752], Loss: 0.9113\n",
      "Epoch [1/20], Step [465/752], Loss: 0.4225\n",
      "Epoch [1/20], Step [466/752], Loss: 0.7657\n",
      "Epoch [1/20], Step [467/752], Loss: 0.4938\n",
      "Epoch [1/20], Step [468/752], Loss: 0.4890\n",
      "Epoch [1/20], Step [469/752], Loss: 1.1173\n",
      "Epoch [1/20], Step [470/752], Loss: 0.3218\n",
      "Epoch [1/20], Step [471/752], Loss: 0.4572\n",
      "Epoch [1/20], Step [472/752], Loss: 0.5467\n",
      "Epoch [1/20], Step [473/752], Loss: 0.3585\n",
      "Epoch [1/20], Step [474/752], Loss: 0.9569\n",
      "Epoch [1/20], Step [475/752], Loss: 1.4285\n",
      "Epoch [1/20], Step [476/752], Loss: 0.4797\n",
      "Epoch [1/20], Step [477/752], Loss: 1.0022\n",
      "Epoch [1/20], Step [478/752], Loss: 0.7194\n",
      "Epoch [1/20], Step [479/752], Loss: 0.9522\n",
      "Epoch [1/20], Step [480/752], Loss: 0.5224\n",
      "Epoch [1/20], Step [481/752], Loss: 0.4884\n",
      "Epoch [1/20], Step [482/752], Loss: 1.8655\n",
      "Epoch [1/20], Step [483/752], Loss: 0.8480\n",
      "Epoch [1/20], Step [484/752], Loss: 0.6228\n",
      "Epoch [1/20], Step [485/752], Loss: 0.3737\n",
      "Epoch [1/20], Step [486/752], Loss: 0.3952\n",
      "Epoch [1/20], Step [487/752], Loss: 0.6707\n",
      "Epoch [1/20], Step [488/752], Loss: 0.7559\n",
      "Epoch [1/20], Step [489/752], Loss: 0.5135\n",
      "Epoch [1/20], Step [490/752], Loss: 0.6749\n",
      "Epoch [1/20], Step [491/752], Loss: 0.5910\n",
      "Epoch [1/20], Step [492/752], Loss: 0.2298\n",
      "Epoch [1/20], Step [493/752], Loss: 0.2247\n",
      "Epoch [1/20], Step [494/752], Loss: 0.2924\n",
      "Epoch [1/20], Step [495/752], Loss: 0.5402\n",
      "Epoch [1/20], Step [496/752], Loss: 0.4910\n",
      "Epoch [1/20], Step [497/752], Loss: 0.3564\n",
      "Epoch [1/20], Step [498/752], Loss: 0.2564\n",
      "Epoch [1/20], Step [499/752], Loss: 0.9090\n",
      "Epoch [1/20], Step [500/752], Loss: 0.3837\n",
      "Epoch [1/20], Step [501/752], Loss: 0.5484\n",
      "Epoch [1/20], Step [502/752], Loss: 0.8041\n",
      "Epoch [1/20], Step [503/752], Loss: 0.7189\n",
      "Epoch [1/20], Step [504/752], Loss: 0.5227\n",
      "Epoch [1/20], Step [505/752], Loss: 0.7178\n",
      "Epoch [1/20], Step [506/752], Loss: 0.6629\n",
      "Epoch [1/20], Step [507/752], Loss: 0.4798\n",
      "Epoch [1/20], Step [508/752], Loss: 0.7278\n",
      "Epoch [1/20], Step [509/752], Loss: 0.4904\n",
      "Epoch [1/20], Step [510/752], Loss: 0.4383\n",
      "Epoch [1/20], Step [511/752], Loss: 0.4996\n",
      "Epoch [1/20], Step [512/752], Loss: 0.1577\n",
      "Epoch [1/20], Step [513/752], Loss: 0.7205\n",
      "Epoch [1/20], Step [514/752], Loss: 0.4744\n",
      "Epoch [1/20], Step [515/752], Loss: 0.4843\n",
      "Epoch [1/20], Step [516/752], Loss: 0.7222\n",
      "Epoch [1/20], Step [517/752], Loss: 1.0293\n",
      "Epoch [1/20], Step [518/752], Loss: 0.4697\n",
      "Epoch [1/20], Step [519/752], Loss: 0.6353\n",
      "Epoch [1/20], Step [520/752], Loss: 0.2826\n",
      "Epoch [1/20], Step [521/752], Loss: 0.5803\n",
      "Epoch [1/20], Step [522/752], Loss: 0.2912\n",
      "Epoch [1/20], Step [523/752], Loss: 1.0289\n",
      "Epoch [1/20], Step [524/752], Loss: 0.5998\n",
      "Epoch [1/20], Step [525/752], Loss: 0.3569\n",
      "Epoch [1/20], Step [526/752], Loss: 0.5301\n",
      "Epoch [1/20], Step [527/752], Loss: 0.3048\n",
      "Epoch [1/20], Step [528/752], Loss: 0.5138\n",
      "Epoch [1/20], Step [529/752], Loss: 0.9961\n",
      "Epoch [1/20], Step [530/752], Loss: 1.3964\n",
      "Epoch [1/20], Step [531/752], Loss: 0.2881\n",
      "Epoch [1/20], Step [532/752], Loss: 0.2194\n",
      "Epoch [1/20], Step [533/752], Loss: 0.4885\n",
      "Epoch [1/20], Step [534/752], Loss: 0.5332\n",
      "Epoch [1/20], Step [535/752], Loss: 0.2217\n",
      "Epoch [1/20], Step [536/752], Loss: 1.1194\n",
      "Epoch [1/20], Step [537/752], Loss: 0.4939\n",
      "Epoch [1/20], Step [538/752], Loss: 0.7355\n",
      "Epoch [1/20], Step [539/752], Loss: 0.4086\n",
      "Epoch [1/20], Step [540/752], Loss: 0.6558\n",
      "Epoch [1/20], Step [541/752], Loss: 0.6124\n",
      "Epoch [1/20], Step [542/752], Loss: 0.4181\n",
      "Epoch [1/20], Step [543/752], Loss: 0.6789\n",
      "Epoch [1/20], Step [544/752], Loss: 0.6763\n",
      "Epoch [1/20], Step [545/752], Loss: 1.0290\n",
      "Epoch [1/20], Step [546/752], Loss: 0.5249\n",
      "Epoch [1/20], Step [547/752], Loss: 0.7930\n",
      "Epoch [1/20], Step [548/752], Loss: 0.5827\n",
      "Epoch [1/20], Step [549/752], Loss: 0.2352\n",
      "Epoch [1/20], Step [550/752], Loss: 1.2442\n",
      "Epoch [1/20], Step [551/752], Loss: 0.6901\n",
      "Epoch [1/20], Step [552/752], Loss: 0.2602\n",
      "Epoch [1/20], Step [553/752], Loss: 1.0237\n",
      "Epoch [1/20], Step [554/752], Loss: 0.6633\n",
      "Epoch [1/20], Step [555/752], Loss: 1.0674\n",
      "Epoch [1/20], Step [556/752], Loss: 0.7156\n",
      "Epoch [1/20], Step [557/752], Loss: 0.4405\n",
      "Epoch [1/20], Step [558/752], Loss: 0.4827\n",
      "Epoch [1/20], Step [559/752], Loss: 0.4203\n",
      "Epoch [1/20], Step [560/752], Loss: 0.7116\n",
      "Epoch [1/20], Step [561/752], Loss: 0.6418\n",
      "Epoch [1/20], Step [562/752], Loss: 0.6038\n",
      "Epoch [1/20], Step [563/752], Loss: 0.4715\n",
      "Epoch [1/20], Step [564/752], Loss: 0.4420\n",
      "Epoch [1/20], Step [565/752], Loss: 0.4239\n",
      "Epoch [1/20], Step [566/752], Loss: 0.3837\n",
      "Epoch [1/20], Step [567/752], Loss: 0.2995\n",
      "Epoch [1/20], Step [568/752], Loss: 0.3842\n",
      "Epoch [1/20], Step [569/752], Loss: 0.8111\n",
      "Epoch [1/20], Step [570/752], Loss: 0.4088\n",
      "Epoch [1/20], Step [571/752], Loss: 0.4415\n",
      "Epoch [1/20], Step [572/752], Loss: 0.8352\n",
      "Epoch [1/20], Step [573/752], Loss: 0.5493\n",
      "Epoch [1/20], Step [574/752], Loss: 0.6238\n",
      "Epoch [1/20], Step [575/752], Loss: 0.5230\n",
      "Epoch [1/20], Step [576/752], Loss: 0.7725\n",
      "Epoch [1/20], Step [577/752], Loss: 0.7669\n",
      "Epoch [1/20], Step [578/752], Loss: 0.7362\n",
      "Epoch [1/20], Step [579/752], Loss: 0.2487\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [580/752], Loss: 1.2889\n",
      "Epoch [1/20], Step [581/752], Loss: 0.2972\n",
      "Epoch [1/20], Step [582/752], Loss: 0.5960\n",
      "Epoch [1/20], Step [583/752], Loss: 0.4924\n",
      "Epoch [1/20], Step [584/752], Loss: 0.4980\n",
      "Epoch [1/20], Step [585/752], Loss: 0.2254\n",
      "Epoch [1/20], Step [586/752], Loss: 0.4359\n",
      "Epoch [1/20], Step [587/752], Loss: 0.3220\n",
      "Epoch [1/20], Step [588/752], Loss: 0.9179\n",
      "Epoch [1/20], Step [589/752], Loss: 0.4271\n",
      "Epoch [1/20], Step [590/752], Loss: 1.1367\n",
      "Epoch [1/20], Step [591/752], Loss: 0.3233\n",
      "Epoch [1/20], Step [592/752], Loss: 1.3435\n",
      "Epoch [1/20], Step [593/752], Loss: 1.2401\n",
      "Epoch [1/20], Step [594/752], Loss: 0.4777\n",
      "Epoch [1/20], Step [595/752], Loss: 0.6038\n",
      "Epoch [1/20], Step [596/752], Loss: 0.6501\n",
      "Epoch [1/20], Step [597/752], Loss: 1.1783\n",
      "Epoch [1/20], Step [598/752], Loss: 0.5592\n",
      "Epoch [1/20], Step [599/752], Loss: 1.5792\n",
      "Epoch [1/20], Step [600/752], Loss: 0.7864\n",
      "Epoch [1/20], Step [601/752], Loss: 0.9965\n",
      "Epoch [1/20], Step [602/752], Loss: 0.8275\n",
      "Epoch [1/20], Step [603/752], Loss: 0.6446\n",
      "Epoch [1/20], Step [604/752], Loss: 0.4051\n",
      "Epoch [1/20], Step [605/752], Loss: 0.6743\n",
      "Epoch [1/20], Step [606/752], Loss: 1.9591\n",
      "Epoch [1/20], Step [607/752], Loss: 0.2874\n",
      "Epoch [1/20], Step [608/752], Loss: 0.3541\n",
      "Epoch [1/20], Step [609/752], Loss: 0.3530\n",
      "Epoch [1/20], Step [610/752], Loss: 0.8980\n",
      "Epoch [1/20], Step [611/752], Loss: 0.7478\n",
      "Epoch [1/20], Step [612/752], Loss: 0.6388\n",
      "Epoch [1/20], Step [613/752], Loss: 0.5507\n",
      "Epoch [1/20], Step [614/752], Loss: 1.1057\n",
      "Epoch [1/20], Step [615/752], Loss: 0.5485\n",
      "Epoch [1/20], Step [616/752], Loss: 0.1660\n",
      "Epoch [1/20], Step [617/752], Loss: 0.5195\n",
      "Epoch [1/20], Step [618/752], Loss: 0.5402\n",
      "Epoch [1/20], Step [619/752], Loss: 1.0502\n",
      "Epoch [1/20], Step [620/752], Loss: 0.7000\n",
      "Epoch [1/20], Step [621/752], Loss: 0.7293\n",
      "Epoch [1/20], Step [622/752], Loss: 0.3756\n",
      "Epoch [1/20], Step [623/752], Loss: 0.1973\n",
      "Epoch [1/20], Step [624/752], Loss: 0.7227\n",
      "Epoch [1/20], Step [625/752], Loss: 0.7916\n",
      "Epoch [1/20], Step [626/752], Loss: 0.5255\n",
      "Epoch [1/20], Step [627/752], Loss: 1.0906\n",
      "Epoch [1/20], Step [628/752], Loss: 0.8298\n",
      "Epoch [1/20], Step [629/752], Loss: 0.5889\n",
      "Epoch [1/20], Step [630/752], Loss: 1.0341\n",
      "Epoch [1/20], Step [631/752], Loss: 0.3358\n",
      "Epoch [1/20], Step [632/752], Loss: 0.5938\n",
      "Epoch [1/20], Step [633/752], Loss: 0.6232\n",
      "Epoch [1/20], Step [634/752], Loss: 0.4924\n",
      "Epoch [1/20], Step [635/752], Loss: 0.6251\n",
      "Epoch [1/20], Step [636/752], Loss: 0.4234\n",
      "Epoch [1/20], Step [637/752], Loss: 0.3353\n",
      "Epoch [1/20], Step [638/752], Loss: 0.3320\n",
      "Epoch [1/20], Step [639/752], Loss: 0.5444\n",
      "Epoch [1/20], Step [640/752], Loss: 1.4882\n",
      "Epoch [1/20], Step [641/752], Loss: 0.5994\n",
      "Epoch [1/20], Step [642/752], Loss: 0.7433\n",
      "Epoch [1/20], Step [643/752], Loss: 0.3606\n",
      "Epoch [1/20], Step [644/752], Loss: 0.6131\n",
      "Epoch [1/20], Step [645/752], Loss: 0.5932\n",
      "Epoch [1/20], Step [646/752], Loss: 0.1741\n",
      "Epoch [1/20], Step [647/752], Loss: 0.2413\n",
      "Epoch [1/20], Step [648/752], Loss: 0.3333\n",
      "Epoch [1/20], Step [649/752], Loss: 1.0123\n",
      "Epoch [1/20], Step [650/752], Loss: 0.4961\n",
      "Epoch [1/20], Step [651/752], Loss: 0.9659\n",
      "Epoch [1/20], Step [652/752], Loss: 0.5152\n",
      "Epoch [1/20], Step [653/752], Loss: 0.5203\n",
      "Epoch [1/20], Step [654/752], Loss: 0.2029\n",
      "Epoch [1/20], Step [655/752], Loss: 0.4227\n",
      "Epoch [1/20], Step [656/752], Loss: 1.3421\n",
      "Epoch [1/20], Step [657/752], Loss: 0.4281\n",
      "Epoch [1/20], Step [658/752], Loss: 0.2395\n",
      "Epoch [1/20], Step [659/752], Loss: 0.5079\n",
      "Epoch [1/20], Step [660/752], Loss: 0.5480\n",
      "Epoch [1/20], Step [661/752], Loss: 0.5536\n",
      "Epoch [1/20], Step [662/752], Loss: 0.2759\n",
      "Epoch [1/20], Step [663/752], Loss: 0.3750\n",
      "Epoch [1/20], Step [664/752], Loss: 0.3254\n",
      "Epoch [1/20], Step [665/752], Loss: 0.6593\n",
      "Epoch [1/20], Step [666/752], Loss: 0.6359\n",
      "Epoch [1/20], Step [667/752], Loss: 0.6180\n",
      "Epoch [1/20], Step [668/752], Loss: 0.6852\n",
      "Epoch [1/20], Step [669/752], Loss: 0.6719\n",
      "Epoch [1/20], Step [670/752], Loss: 0.9559\n",
      "Epoch [1/20], Step [671/752], Loss: 0.5998\n",
      "Epoch [1/20], Step [672/752], Loss: 0.5261\n",
      "Epoch [1/20], Step [673/752], Loss: 0.3465\n",
      "Epoch [1/20], Step [674/752], Loss: 1.2775\n",
      "Epoch [1/20], Step [675/752], Loss: 0.2693\n",
      "Epoch [1/20], Step [676/752], Loss: 0.3399\n",
      "Epoch [1/20], Step [677/752], Loss: 0.9456\n",
      "Epoch [1/20], Step [678/752], Loss: 0.2389\n",
      "Epoch [1/20], Step [679/752], Loss: 0.1908\n",
      "Epoch [1/20], Step [680/752], Loss: 0.3095\n",
      "Epoch [1/20], Step [681/752], Loss: 0.7652\n",
      "Epoch [1/20], Step [682/752], Loss: 0.2307\n",
      "Epoch [1/20], Step [683/752], Loss: 0.6230\n",
      "Epoch [1/20], Step [684/752], Loss: 1.0565\n",
      "Epoch [1/20], Step [685/752], Loss: 0.2326\n",
      "Epoch [1/20], Step [686/752], Loss: 0.5077\n",
      "Epoch [1/20], Step [687/752], Loss: 0.8494\n",
      "Epoch [1/20], Step [688/752], Loss: 0.7246\n",
      "Epoch [1/20], Step [689/752], Loss: 0.7454\n",
      "Epoch [1/20], Step [690/752], Loss: 0.7179\n",
      "Epoch [1/20], Step [691/752], Loss: 0.3652\n",
      "Epoch [1/20], Step [692/752], Loss: 0.5071\n",
      "Epoch [1/20], Step [693/752], Loss: 0.5542\n",
      "Epoch [1/20], Step [694/752], Loss: 0.8449\n",
      "Epoch [1/20], Step [695/752], Loss: 0.7664\n",
      "Epoch [1/20], Step [696/752], Loss: 0.7253\n",
      "Epoch [1/20], Step [697/752], Loss: 0.2593\n",
      "Epoch [1/20], Step [698/752], Loss: 0.6404\n",
      "Epoch [1/20], Step [699/752], Loss: 0.2463\n",
      "Epoch [1/20], Step [700/752], Loss: 0.7806\n",
      "Epoch [1/20], Step [701/752], Loss: 0.6780\n",
      "Epoch [1/20], Step [702/752], Loss: 0.8297\n",
      "Epoch [1/20], Step [703/752], Loss: 0.4554\n",
      "Epoch [1/20], Step [704/752], Loss: 0.6342\n",
      "Epoch [1/20], Step [705/752], Loss: 0.1727\n",
      "Epoch [1/20], Step [706/752], Loss: 0.5112\n",
      "Epoch [1/20], Step [707/752], Loss: 0.5438\n",
      "Epoch [1/20], Step [708/752], Loss: 0.3654\n",
      "Epoch [1/20], Step [709/752], Loss: 0.4277\n",
      "Epoch [1/20], Step [710/752], Loss: 0.5215\n",
      "Epoch [1/20], Step [711/752], Loss: 0.4555\n",
      "Epoch [1/20], Step [712/752], Loss: 0.9942\n",
      "Epoch [1/20], Step [713/752], Loss: 0.3757\n",
      "Epoch [1/20], Step [714/752], Loss: 0.6789\n",
      "Epoch [1/20], Step [715/752], Loss: 1.0270\n",
      "Epoch [1/20], Step [716/752], Loss: 0.6403\n",
      "Epoch [1/20], Step [717/752], Loss: 0.3352\n",
      "Epoch [1/20], Step [718/752], Loss: 0.4738\n",
      "Epoch [1/20], Step [719/752], Loss: 0.4062\n",
      "Epoch [1/20], Step [720/752], Loss: 0.7551\n",
      "Epoch [1/20], Step [721/752], Loss: 0.9469\n",
      "Epoch [1/20], Step [722/752], Loss: 0.7716\n",
      "Epoch [1/20], Step [723/752], Loss: 0.6884\n",
      "Epoch [1/20], Step [724/752], Loss: 0.6944\n",
      "Epoch [1/20], Step [725/752], Loss: 0.8514\n",
      "Epoch [1/20], Step [726/752], Loss: 0.3548\n",
      "Epoch [1/20], Step [727/752], Loss: 0.4007\n",
      "Epoch [1/20], Step [728/752], Loss: 0.6999\n",
      "Epoch [1/20], Step [729/752], Loss: 0.5379\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_mse = 1000\n",
    "best_ci = 0\n",
    "model_file_name = 'best_sim-CNN-DTA_davis.model'\n",
    "result_file_name = 'best_result_sim-CNNDTA_davis.csv'\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    c=0\n",
    "    for i in train_loader:\n",
    "        c=c+1\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "           \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, c, total_step, loss.item()))\n",
    "    \n",
    "    # taking best model so far\n",
    "    G,P = predicting(model, device, test_loader)\n",
    "    ret = [rmse(G, P), mse(G, P), pearson(G, P), ci(G, P)]\n",
    "    if ret[1] < best_mse:\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        with open(result_file_name, 'w') as f:\n",
    "            f.write(','.join(map(str, ret)))\n",
    "        best_epoch = epoch+1\n",
    "        best_mse = ret[1]\n",
    "        best_ci = ret[-1]\n",
    "        best_r2 = ret[2]\n",
    "        \n",
    "        print('rmse improved at epoch ', best_epoch,\n",
    "                      '; best_mse,best_ci,best_r2:', best_mse, best_ci,best_r2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47c54c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c068b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b23b7bbd",
   "metadata": {},
   "source": [
    "# Testing model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec50a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "total_preds = np.array([])\n",
    "total_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in test_loader:\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images) \n",
    "        outputs = outputs.cpu().detach().numpy().flatten()\n",
    "        labels =labels.cpu().detach().numpy().flatten()\n",
    "        total_preds = np.concatenate([total_preds, outputs])\n",
    "        total_labels = np.concatenate([total_labels, labels])\n",
    "#         total_preds = torch.cat(total_preds, outputs.cpu(), 0 )\n",
    "#         total_labels = torch.cat(total_labels, labels.cpu(), 0)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb176c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(total_labels, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878d20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(total_labels, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776942c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson(total_labels, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e12de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci(total_labels, total_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03fd298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
