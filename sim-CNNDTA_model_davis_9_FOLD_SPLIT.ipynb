{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "537b4fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import json,pickle,math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f93d3ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LSM = pickle.load(open('./davis_ligand_similarity_matrix.pkl', 'rb'))\n",
    "PSM = pickle.load(open('./davis_protein_similarity_matrix.pkl', 'rb'))\n",
    "full_df = pd.read_csv(open('./davis_all_pairs.csv','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff51505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b32f117",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMILES = json.load(open('./data/DAVIS/SMILES.txt'))\n",
    "TARGETS = json.load(open('./data/DAVIS/target_seq.txt'))\n",
    "SMILES=list(SMILES.values())\n",
    "TARGETS=list(TARGETS.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46ae1d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_9_folds={}\n",
    "for i in [0,1,2]:\n",
    "    for j in [0,1,2]:\n",
    "        file_name = 'fold' +str(i) +str(j) \n",
    "        \n",
    "        temp = open('./data/davis/DAVIS_9_FOLDS/' + file_name +'.pkl', 'rb')\n",
    "        new_df = pd.read_pickle(temp)\n",
    "        all_9_folds.update({file_name:new_df})\n",
    "        temp.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09356acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_davis_test_train(test_fold_number,all_9_folds):\n",
    "    test_protein_fold_id = test_fold_number[0]\n",
    "    test_ligand_fold_id = test_fold_number[1]\n",
    "    test_set = pd.DataFrame(columns = full_df.columns)\n",
    "    train_set = pd.DataFrame(columns= full_df.columns)\n",
    "    for i in [0,1,2]:\n",
    "        for j in [0,1,2]:\n",
    "            fold_name = 'fold' + str(i) + str(j)\n",
    "            df = all_9_folds[fold_name]\n",
    "            \n",
    "            if str(i) == test_protein_fold_id and str(j) == test_ligand_fold_id:\n",
    "                test_set = df.copy()\n",
    "                \n",
    "            if str(i) != test_protein_fold_id and str(j) != test_ligand_fold_id:\n",
    "                train_set = pd.concat([train_set, df.copy()], ignore_index=True)\n",
    "                \n",
    "                \n",
    "    return train_set, test_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fc561",
   "metadata": {},
   "source": [
    "# Create train test split on these 9 folds\n",
    "## fold_number is the id of fold. For example, test = fold00, train = fold 11,22,12,21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "781f08f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_number = '01'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dacc3b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = create_davis_test_train(test_fold_number=fold_number, all_9_folds=all_9_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29c5a32",
   "metadata": {},
   "source": [
    "# To ensure that there are no common targets or drugs in train and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4e8c2b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_smiles = list(test['SMILES'])\n",
    "test_targets = list(test['Target Sequence'])\n",
    "train_smiles = list(train['SMILES'])\n",
    "train_targets = list(train['Target Sequence'])\n",
    "\n",
    "for i in test_smiles:\n",
    "    if i in train_smiles:\n",
    "        print(\"common entity present\")\n",
    "for i in test_targets:\n",
    "    if i in train_targets:\n",
    "        print(\"common entity present\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0715faa4",
   "metadata": {},
   "source": [
    "# Creating outer products for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d06979a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3404, 1, 68, 442)\n"
     ]
    }
   ],
   "source": [
    "outer_test_prods = []\n",
    "for i,row in test.iterrows():\n",
    "#     print(i)\n",
    "    smi = row['SMILES']\n",
    "    seq = row['Target Sequence']\n",
    "    target_id = TARGETS.index(seq)\n",
    "    smi_id = SMILES.index(smi)\n",
    "    ki=LSM[smi_id]\n",
    "    kj=PSM[target_id]\n",
    "    ki_x_kj = np.outer(ki,kj)\n",
    "    outer_test_prods.append([ki_x_kj])\n",
    "outer_test_prods = np.array(outer_test_prods)\n",
    "print(np.shape(outer_test_prods))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfaeed1",
   "metadata": {},
   "source": [
    "# creating outer products for train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fd73f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13230, 1, 68, 442)\n"
     ]
    }
   ],
   "source": [
    "outer_train_prods = []\n",
    "for i,row in train.iterrows():\n",
    "#     print(i)\n",
    "    smi = row['SMILES']\n",
    "    seq = row['Target Sequence']\n",
    "    target_id = TARGETS.index(seq)\n",
    "    smi_id = SMILES.index(smi)\n",
    "    ki=LSM[smi_id]\n",
    "    kj=PSM[target_id]\n",
    "    ki_x_kj = np.outer(ki,kj)\n",
    "    outer_train_prods.append([ki_x_kj])\n",
    "outer_train_prods = np.array(outer_train_prods)\n",
    "print(np.shape(outer_train_prods))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5db8673f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper parameters\n",
    "num_epochs = 20\n",
    "# num_classes = 10\n",
    "batch_size = 32\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1531a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe, outer_prods, transform=None):\n",
    "#         self.df = pd.read_csv(open(csv_file))\n",
    "        self.df = dataframe\n",
    "#         self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.outer_prods = outer_prods\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        output = {'outer_product': self.outer_prods[idx] , 'Label':self.df.iloc[idx]['Label']}\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5111d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = custom_dataset(dataframe = train, outer_prods = outer_train_prods)\n",
    "test_dataset = custom_dataset(dataframe = test, outer_prods = outer_test_prods)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64ef51fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader= torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader= torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f9bf180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13248 3424\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)*32, len(test_loader)*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "488727be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32, 5).double()\n",
    "        self.pool1 = nn.MaxPool2d(2,2).double()\n",
    "        self.conv2 = nn.Conv2d(32,18,3).double()\n",
    "        self.pool2 = nn.MaxPool2d(2,2).double()\n",
    "        self.fc1 = nn.Linear(18*15*108, 128).double()\n",
    "        self.fc2 = nn.Linear(128,1).double()\n",
    "        self.dropout = nn.Dropout(0.1).double()\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(-1,18*15*108)\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd246053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in test_loader:\n",
    "#     a = i['outer_product']\n",
    "#     b= i['Label']\n",
    "#     break\n",
    "# conv1 = nn.Conv2d(1,32,5).double()\n",
    "# pool = nn.MaxPool2d(2,2).double()\n",
    "# conv2 = nn.Conv2d(32,18,3).double()\n",
    "# fc1 = nn.Linear(18*15*108, 128).double()\n",
    "# fc2 = nn.Linear(128,1).double()\n",
    "# dropout = nn.Dropout(0.1).double()\n",
    "# x= conv1(a)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x= conv2(x)\n",
    "# print(x.shape)\n",
    "# x = pool(x)\n",
    "# print(x.shape)\n",
    "# x = x.view(-1,18*15*108)\n",
    "# print(x.shape)\n",
    "# x = dropout(x)\n",
    "# print(x.shape)\n",
    "# x = fc1(x)\n",
    "# print(x.shape)\n",
    "# x = fc2(x)\n",
    "# print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eaeb26dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ad99a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377b27e",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "746665a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y,f):\n",
    "    rmse = math.sqrt(((y - f)**2).mean(axis=0))\n",
    "    return rmse\n",
    "def mse(y,f):\n",
    "    mse = ((y - f)**2).mean(axis=0)\n",
    "    return mse\n",
    "def pearson(y,f):\n",
    "    rp = np.corrcoef(y, f)[0,1]\n",
    "    return rp\n",
    "from lifelines.utils import concordance_index\n",
    "def ci(y,f):\n",
    "    return concordance_index(y,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "816a59e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predicting(model, device, test_loader):\n",
    "    model.eval()\n",
    "    total_preds = np.array([])\n",
    "    total_labels = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i in test_loader:\n",
    "            images = i['outer_product']\n",
    "            labels = i['Label']\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images) \n",
    "            outputs = outputs.cpu().detach().numpy().flatten()\n",
    "            labels =labels.cpu().detach().numpy().flatten()\n",
    "            total_preds = np.concatenate([total_preds, outputs])\n",
    "            total_labels = np.concatenate([total_labels, labels])\n",
    "    \n",
    "    model.train()\n",
    "    return total_labels, total_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55973fd",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89ffb693",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file_name = './SAVED_MODELS/best_sim-CNN-DTA_davis_fold' + fold_number +  '.model'\n",
    "result_file_name = './SAVED_MODELS/best_result_sim-CNNDTA_davis_fold'+fold_number + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e9187",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home2/kanakala.ganesh/miniconda3/envs/geometric/lib/python3.9/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448238472/work/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [1/414], Loss: 30.1050\n",
      "Epoch [1/20], Step [2/414], Loss: 14.3208\n",
      "Epoch [1/20], Step [3/414], Loss: 1.9628\n",
      "Epoch [1/20], Step [4/414], Loss: 15.5687\n",
      "Epoch [1/20], Step [5/414], Loss: 5.3748\n",
      "Epoch [1/20], Step [6/414], Loss: 1.1842\n",
      "Epoch [1/20], Step [7/414], Loss: 1.8497\n",
      "Epoch [1/20], Step [8/414], Loss: 3.3616\n",
      "Epoch [1/20], Step [9/414], Loss: 3.8290\n",
      "Epoch [1/20], Step [10/414], Loss: 3.8511\n",
      "Epoch [1/20], Step [11/414], Loss: 1.9314\n",
      "Epoch [1/20], Step [12/414], Loss: 0.7047\n",
      "Epoch [1/20], Step [13/414], Loss: 0.4676\n",
      "Epoch [1/20], Step [14/414], Loss: 1.2022\n",
      "Epoch [1/20], Step [15/414], Loss: 1.6717\n",
      "Epoch [1/20], Step [16/414], Loss: 2.1351\n",
      "Epoch [1/20], Step [17/414], Loss: 1.5269\n",
      "Epoch [1/20], Step [18/414], Loss: 1.8639\n",
      "Epoch [1/20], Step [19/414], Loss: 1.2276\n",
      "Epoch [1/20], Step [20/414], Loss: 1.5450\n",
      "Epoch [1/20], Step [21/414], Loss: 1.1548\n",
      "Epoch [1/20], Step [22/414], Loss: 1.6819\n",
      "Epoch [1/20], Step [23/414], Loss: 0.7274\n",
      "Epoch [1/20], Step [24/414], Loss: 0.7713\n",
      "Epoch [1/20], Step [25/414], Loss: 0.5857\n",
      "Epoch [1/20], Step [26/414], Loss: 0.8607\n",
      "Epoch [1/20], Step [27/414], Loss: 1.0261\n",
      "Epoch [1/20], Step [28/414], Loss: 1.7852\n",
      "Epoch [1/20], Step [29/414], Loss: 1.2278\n",
      "Epoch [1/20], Step [30/414], Loss: 0.9364\n",
      "Epoch [1/20], Step [31/414], Loss: 0.6603\n",
      "Epoch [1/20], Step [32/414], Loss: 0.5743\n",
      "Epoch [1/20], Step [33/414], Loss: 0.9013\n",
      "Epoch [1/20], Step [34/414], Loss: 0.7469\n",
      "Epoch [1/20], Step [35/414], Loss: 0.9380\n",
      "Epoch [1/20], Step [36/414], Loss: 1.2391\n",
      "Epoch [1/20], Step [37/414], Loss: 0.3377\n",
      "Epoch [1/20], Step [38/414], Loss: 1.0930\n",
      "Epoch [1/20], Step [39/414], Loss: 0.4876\n",
      "Epoch [1/20], Step [40/414], Loss: 0.7734\n",
      "Epoch [1/20], Step [41/414], Loss: 1.1689\n",
      "Epoch [1/20], Step [42/414], Loss: 1.0099\n",
      "Epoch [1/20], Step [43/414], Loss: 1.5012\n",
      "Epoch [1/20], Step [44/414], Loss: 0.8024\n",
      "Epoch [1/20], Step [45/414], Loss: 1.1155\n",
      "Epoch [1/20], Step [46/414], Loss: 1.6615\n",
      "Epoch [1/20], Step [47/414], Loss: 0.3511\n",
      "Epoch [1/20], Step [48/414], Loss: 0.9417\n",
      "Epoch [1/20], Step [49/414], Loss: 0.4547\n",
      "Epoch [1/20], Step [50/414], Loss: 1.5954\n",
      "Epoch [1/20], Step [51/414], Loss: 0.8761\n",
      "Epoch [1/20], Step [52/414], Loss: 1.0606\n",
      "Epoch [1/20], Step [53/414], Loss: 1.2571\n",
      "Epoch [1/20], Step [54/414], Loss: 1.2096\n",
      "Epoch [1/20], Step [55/414], Loss: 0.8271\n",
      "Epoch [1/20], Step [56/414], Loss: 0.9068\n",
      "Epoch [1/20], Step [57/414], Loss: 0.7877\n",
      "Epoch [1/20], Step [58/414], Loss: 0.2841\n",
      "Epoch [1/20], Step [59/414], Loss: 1.1610\n",
      "Epoch [1/20], Step [60/414], Loss: 0.9220\n",
      "Epoch [1/20], Step [61/414], Loss: 0.3367\n",
      "Epoch [1/20], Step [62/414], Loss: 0.8959\n",
      "Epoch [1/20], Step [63/414], Loss: 1.7305\n",
      "Epoch [1/20], Step [64/414], Loss: 1.2619\n",
      "Epoch [1/20], Step [65/414], Loss: 1.6412\n",
      "Epoch [1/20], Step [66/414], Loss: 1.0935\n",
      "Epoch [1/20], Step [67/414], Loss: 1.6480\n",
      "Epoch [1/20], Step [68/414], Loss: 1.0180\n",
      "Epoch [1/20], Step [69/414], Loss: 0.6558\n",
      "Epoch [1/20], Step [70/414], Loss: 1.2781\n",
      "Epoch [1/20], Step [71/414], Loss: 1.4794\n",
      "Epoch [1/20], Step [72/414], Loss: 0.7786\n",
      "Epoch [1/20], Step [73/414], Loss: 0.5260\n",
      "Epoch [1/20], Step [74/414], Loss: 1.4244\n",
      "Epoch [1/20], Step [75/414], Loss: 0.8811\n",
      "Epoch [1/20], Step [76/414], Loss: 1.6273\n",
      "Epoch [1/20], Step [77/414], Loss: 0.8695\n",
      "Epoch [1/20], Step [78/414], Loss: 0.5921\n",
      "Epoch [1/20], Step [79/414], Loss: 1.6462\n",
      "Epoch [1/20], Step [80/414], Loss: 1.1449\n",
      "Epoch [1/20], Step [81/414], Loss: 1.1491\n",
      "Epoch [1/20], Step [82/414], Loss: 0.5910\n",
      "Epoch [1/20], Step [83/414], Loss: 0.7185\n",
      "Epoch [1/20], Step [84/414], Loss: 0.6431\n",
      "Epoch [1/20], Step [85/414], Loss: 0.6878\n",
      "Epoch [1/20], Step [86/414], Loss: 0.9384\n",
      "Epoch [1/20], Step [87/414], Loss: 0.6934\n",
      "Epoch [1/20], Step [88/414], Loss: 1.2996\n",
      "Epoch [1/20], Step [89/414], Loss: 0.7401\n",
      "Epoch [1/20], Step [90/414], Loss: 1.3039\n",
      "Epoch [1/20], Step [91/414], Loss: 0.6022\n",
      "Epoch [1/20], Step [92/414], Loss: 1.3938\n",
      "Epoch [1/20], Step [93/414], Loss: 0.9887\n",
      "Epoch [1/20], Step [94/414], Loss: 0.6864\n",
      "Epoch [1/20], Step [95/414], Loss: 0.9485\n",
      "Epoch [1/20], Step [96/414], Loss: 0.7304\n",
      "Epoch [1/20], Step [97/414], Loss: 0.4513\n",
      "Epoch [1/20], Step [98/414], Loss: 1.9069\n",
      "Epoch [1/20], Step [99/414], Loss: 1.4022\n",
      "Epoch [1/20], Step [100/414], Loss: 0.9324\n",
      "Epoch [1/20], Step [101/414], Loss: 1.0590\n",
      "Epoch [1/20], Step [102/414], Loss: 0.7373\n",
      "Epoch [1/20], Step [103/414], Loss: 1.5912\n",
      "Epoch [1/20], Step [104/414], Loss: 0.6956\n",
      "Epoch [1/20], Step [105/414], Loss: 1.2001\n",
      "Epoch [1/20], Step [106/414], Loss: 0.7910\n",
      "Epoch [1/20], Step [107/414], Loss: 0.8527\n",
      "Epoch [1/20], Step [108/414], Loss: 0.4437\n",
      "Epoch [1/20], Step [109/414], Loss: 1.3572\n",
      "Epoch [1/20], Step [110/414], Loss: 1.1264\n",
      "Epoch [1/20], Step [111/414], Loss: 0.5313\n",
      "Epoch [1/20], Step [112/414], Loss: 1.1614\n",
      "Epoch [1/20], Step [113/414], Loss: 1.0606\n",
      "Epoch [1/20], Step [114/414], Loss: 0.4946\n",
      "Epoch [1/20], Step [115/414], Loss: 1.3173\n",
      "Epoch [1/20], Step [116/414], Loss: 1.3134\n",
      "Epoch [1/20], Step [117/414], Loss: 1.0988\n",
      "Epoch [1/20], Step [118/414], Loss: 0.7681\n",
      "Epoch [1/20], Step [119/414], Loss: 1.2794\n",
      "Epoch [1/20], Step [120/414], Loss: 1.3186\n",
      "Epoch [1/20], Step [121/414], Loss: 1.3079\n",
      "Epoch [1/20], Step [122/414], Loss: 0.7809\n",
      "Epoch [1/20], Step [123/414], Loss: 0.3556\n",
      "Epoch [1/20], Step [124/414], Loss: 1.0687\n",
      "Epoch [1/20], Step [125/414], Loss: 0.7278\n",
      "Epoch [1/20], Step [126/414], Loss: 0.9077\n",
      "Epoch [1/20], Step [127/414], Loss: 0.4105\n",
      "Epoch [1/20], Step [128/414], Loss: 0.7557\n",
      "Epoch [1/20], Step [129/414], Loss: 0.9933\n",
      "Epoch [1/20], Step [130/414], Loss: 0.8560\n",
      "Epoch [1/20], Step [131/414], Loss: 1.2132\n",
      "Epoch [1/20], Step [132/414], Loss: 0.6527\n",
      "Epoch [1/20], Step [133/414], Loss: 0.2075\n",
      "Epoch [1/20], Step [134/414], Loss: 1.3765\n",
      "Epoch [1/20], Step [135/414], Loss: 1.1285\n",
      "Epoch [1/20], Step [136/414], Loss: 1.1783\n",
      "Epoch [1/20], Step [137/414], Loss: 0.9799\n",
      "Epoch [1/20], Step [138/414], Loss: 0.6610\n",
      "Epoch [1/20], Step [139/414], Loss: 1.4449\n",
      "Epoch [1/20], Step [140/414], Loss: 1.0995\n",
      "Epoch [1/20], Step [141/414], Loss: 0.6464\n",
      "Epoch [1/20], Step [142/414], Loss: 0.7437\n",
      "Epoch [1/20], Step [143/414], Loss: 1.0762\n",
      "Epoch [1/20], Step [144/414], Loss: 0.7058\n",
      "Epoch [1/20], Step [145/414], Loss: 0.8323\n",
      "Epoch [1/20], Step [146/414], Loss: 0.5397\n",
      "Epoch [1/20], Step [147/414], Loss: 1.1918\n",
      "Epoch [1/20], Step [148/414], Loss: 0.6274\n",
      "Epoch [1/20], Step [149/414], Loss: 0.7361\n",
      "Epoch [1/20], Step [150/414], Loss: 0.7692\n",
      "Epoch [1/20], Step [151/414], Loss: 0.6042\n",
      "Epoch [1/20], Step [152/414], Loss: 0.6834\n",
      "Epoch [1/20], Step [153/414], Loss: 0.5918\n",
      "Epoch [1/20], Step [154/414], Loss: 0.9003\n",
      "Epoch [1/20], Step [155/414], Loss: 0.9092\n",
      "Epoch [1/20], Step [156/414], Loss: 0.7961\n",
      "Epoch [1/20], Step [157/414], Loss: 1.0201\n",
      "Epoch [1/20], Step [158/414], Loss: 1.1432\n",
      "Epoch [1/20], Step [159/414], Loss: 0.8523\n",
      "Epoch [1/20], Step [160/414], Loss: 0.6345\n",
      "Epoch [1/20], Step [161/414], Loss: 0.2655\n",
      "Epoch [1/20], Step [162/414], Loss: 1.5019\n",
      "Epoch [1/20], Step [163/414], Loss: 0.1459\n",
      "Epoch [1/20], Step [164/414], Loss: 0.3443\n",
      "Epoch [1/20], Step [165/414], Loss: 0.4522\n",
      "Epoch [1/20], Step [166/414], Loss: 0.8932\n",
      "Epoch [1/20], Step [167/414], Loss: 0.4000\n",
      "Epoch [1/20], Step [168/414], Loss: 1.0167\n",
      "Epoch [1/20], Step [169/414], Loss: 0.3083\n",
      "Epoch [1/20], Step [170/414], Loss: 0.5311\n",
      "Epoch [1/20], Step [171/414], Loss: 0.6135\n",
      "Epoch [1/20], Step [172/414], Loss: 0.9664\n",
      "Epoch [1/20], Step [173/414], Loss: 0.4861\n",
      "Epoch [1/20], Step [174/414], Loss: 0.5766\n",
      "Epoch [1/20], Step [175/414], Loss: 0.3171\n",
      "Epoch [1/20], Step [176/414], Loss: 0.3862\n",
      "Epoch [1/20], Step [177/414], Loss: 0.7248\n",
      "Epoch [1/20], Step [178/414], Loss: 1.1015\n",
      "Epoch [1/20], Step [179/414], Loss: 1.1594\n",
      "Epoch [1/20], Step [180/414], Loss: 1.0856\n",
      "Epoch [1/20], Step [181/414], Loss: 1.0466\n",
      "Epoch [1/20], Step [182/414], Loss: 1.2885\n",
      "Epoch [1/20], Step [183/414], Loss: 0.7269\n",
      "Epoch [1/20], Step [184/414], Loss: 0.5769\n",
      "Epoch [1/20], Step [185/414], Loss: 0.6342\n",
      "Epoch [1/20], Step [186/414], Loss: 1.0775\n",
      "Epoch [1/20], Step [187/414], Loss: 0.3852\n",
      "Epoch [1/20], Step [188/414], Loss: 0.7509\n",
      "Epoch [1/20], Step [189/414], Loss: 0.7218\n",
      "Epoch [1/20], Step [190/414], Loss: 0.6358\n",
      "Epoch [1/20], Step [191/414], Loss: 0.6030\n",
      "Epoch [1/20], Step [192/414], Loss: 0.7207\n",
      "Epoch [1/20], Step [193/414], Loss: 1.0837\n",
      "Epoch [1/20], Step [194/414], Loss: 0.6260\n",
      "Epoch [1/20], Step [195/414], Loss: 0.6788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Step [196/414], Loss: 0.3065\n",
      "Epoch [1/20], Step [197/414], Loss: 0.4238\n",
      "Epoch [1/20], Step [198/414], Loss: 0.4194\n",
      "Epoch [1/20], Step [199/414], Loss: 0.7669\n",
      "Epoch [1/20], Step [200/414], Loss: 1.8194\n",
      "Epoch [1/20], Step [201/414], Loss: 0.6881\n",
      "Epoch [1/20], Step [202/414], Loss: 0.7249\n",
      "Epoch [1/20], Step [203/414], Loss: 0.6969\n",
      "Epoch [1/20], Step [204/414], Loss: 0.3435\n",
      "Epoch [1/20], Step [205/414], Loss: 1.5128\n",
      "Epoch [1/20], Step [206/414], Loss: 0.9814\n",
      "Epoch [1/20], Step [207/414], Loss: 0.8912\n",
      "Epoch [1/20], Step [208/414], Loss: 0.5105\n",
      "Epoch [1/20], Step [209/414], Loss: 0.7950\n",
      "Epoch [1/20], Step [210/414], Loss: 0.7636\n",
      "Epoch [1/20], Step [211/414], Loss: 0.9274\n",
      "Epoch [1/20], Step [212/414], Loss: 0.4985\n",
      "Epoch [1/20], Step [213/414], Loss: 0.4396\n",
      "Epoch [1/20], Step [214/414], Loss: 1.0070\n",
      "Epoch [1/20], Step [215/414], Loss: 0.4322\n",
      "Epoch [1/20], Step [216/414], Loss: 0.6054\n",
      "Epoch [1/20], Step [217/414], Loss: 0.8688\n",
      "Epoch [1/20], Step [218/414], Loss: 0.7744\n",
      "Epoch [1/20], Step [219/414], Loss: 0.6960\n",
      "Epoch [1/20], Step [220/414], Loss: 1.1983\n",
      "Epoch [1/20], Step [221/414], Loss: 0.4548\n",
      "Epoch [1/20], Step [222/414], Loss: 0.7757\n",
      "Epoch [1/20], Step [223/414], Loss: 0.5053\n",
      "Epoch [1/20], Step [224/414], Loss: 0.5382\n",
      "Epoch [1/20], Step [225/414], Loss: 0.4022\n",
      "Epoch [1/20], Step [226/414], Loss: 0.3620\n",
      "Epoch [1/20], Step [227/414], Loss: 0.8317\n",
      "Epoch [1/20], Step [228/414], Loss: 0.3206\n",
      "Epoch [1/20], Step [229/414], Loss: 1.4396\n",
      "Epoch [1/20], Step [230/414], Loss: 0.6201\n",
      "Epoch [1/20], Step [231/414], Loss: 1.2888\n",
      "Epoch [1/20], Step [232/414], Loss: 0.8688\n",
      "Epoch [1/20], Step [233/414], Loss: 0.5370\n",
      "Epoch [1/20], Step [234/414], Loss: 1.1752\n",
      "Epoch [1/20], Step [235/414], Loss: 0.2465\n",
      "Epoch [1/20], Step [236/414], Loss: 0.2092\n",
      "Epoch [1/20], Step [237/414], Loss: 0.5036\n",
      "Epoch [1/20], Step [238/414], Loss: 0.1419\n",
      "Epoch [1/20], Step [239/414], Loss: 0.6975\n",
      "Epoch [1/20], Step [240/414], Loss: 0.6582\n",
      "Epoch [1/20], Step [241/414], Loss: 0.5806\n",
      "Epoch [1/20], Step [242/414], Loss: 0.5894\n",
      "Epoch [1/20], Step [243/414], Loss: 0.7599\n",
      "Epoch [1/20], Step [244/414], Loss: 1.2676\n",
      "Epoch [1/20], Step [245/414], Loss: 0.4743\n",
      "Epoch [1/20], Step [246/414], Loss: 0.6207\n",
      "Epoch [1/20], Step [247/414], Loss: 0.5815\n",
      "Epoch [1/20], Step [248/414], Loss: 1.1008\n",
      "Epoch [1/20], Step [249/414], Loss: 1.1408\n",
      "Epoch [1/20], Step [250/414], Loss: 0.9692\n",
      "Epoch [1/20], Step [251/414], Loss: 0.9366\n",
      "Epoch [1/20], Step [252/414], Loss: 0.8436\n",
      "Epoch [1/20], Step [253/414], Loss: 0.3289\n",
      "Epoch [1/20], Step [254/414], Loss: 2.2025\n",
      "Epoch [1/20], Step [255/414], Loss: 0.9829\n",
      "Epoch [1/20], Step [256/414], Loss: 0.8020\n",
      "Epoch [1/20], Step [257/414], Loss: 0.5732\n",
      "Epoch [1/20], Step [258/414], Loss: 0.5254\n",
      "Epoch [1/20], Step [259/414], Loss: 1.2817\n",
      "Epoch [1/20], Step [260/414], Loss: 1.2186\n",
      "Epoch [1/20], Step [261/414], Loss: 0.5134\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "best_mse = 1000\n",
    "best_ci = 0\n",
    "\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    c=0\n",
    "    for i in train_loader:\n",
    "        c=c+1\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.flatten(), labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "           \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "               .format(epoch+1, num_epochs, c, total_step, loss.item()))\n",
    "    \n",
    "    # taking best model so far\n",
    "    G,P = predicting(model, device, test_loader)\n",
    "    ret = [rmse(G, P), mse(G, P), pearson(G, P), ci(G, P)]\n",
    "    if ret[1] < best_mse:\n",
    "        torch.save(model.state_dict(), model_file_name)\n",
    "        with open(result_file_name, 'w') as f:\n",
    "            f.write(','.join(map(str, ret)))\n",
    "        best_epoch = epoch+1\n",
    "        best_mse = ret[1]\n",
    "        best_ci = ret[-1]\n",
    "        best_r = ret[2]\n",
    "        \n",
    "        print('rmse improved at epoch ', best_epoch,\n",
    "                      '; best_mse,best_ci,best_r:', best_mse, best_ci,best_r)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990f868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113d1063",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "# eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "total_preds = np.array([])\n",
    "total_labels = np.array([])\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in test_loader:\n",
    "        images = i['outer_product']\n",
    "        labels = i['Label']\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images) \n",
    "        outputs = outputs.cpu().detach().numpy().flatten()\n",
    "        labels =labels.cpu().detach().numpy().flatten()\n",
    "        total_preds = np.concatenate([total_preds, outputs])\n",
    "        total_labels = np.concatenate([total_labels, labels])\n",
    "#         total_preds = torch.cat(total_preds, outputs.cpu(), 0 )\n",
    "#         total_labels = torch.cat(total_labels, labels.cpu(), 0)\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3b1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "G,P = total_labels, total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d3c147",
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse(G,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse(G,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4229a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson(G,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31a92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci(G,P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bbdadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75a98b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82194d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
